import json
from dataclasses import dataclass, field
from typing import Optional
from pathlib import Path
import numpy as np
from datasets import load_dataset

from time import perf_counter_ns
import numpy as np
from contextlib import contextmanager

SEC_TO_NS_SCALE = 1000000000

@dataclass
class Benchmark:
    summary_msg: str = field(default_factory=str)

    @property
    def num_runs(self) -> int:
        return len(self.latencies)

    @contextmanager
    def track(self, step):
        start = perf_counter_ns()
        yield
        ns = perf_counter_ns() - start
        msg = f"\n{'*' * 70}\n'{step}' took {ns / SEC_TO_NS_SCALE:.2f}s ({ns:,}ns)\n{'*' * 70}\n"
        print(msg)
        self.summary_msg += msg + '\n'

    def summary(self):
        print(f"\n{'#' * 30}\nBenchmark Summary:\n{'#' * 30}\n\n{self.summary_msg}")

@dataclass
class Arguments:
    """
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
    """
    lm_method: str = field(
        default=None,
        metadata={
            "help": "Few-shot LM Loss method ('pet' / 'adapet')."
        },
    )
    model_name_or_path: str = field(
        default="roberta-base",
        metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
    )

    tokenizer_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
    )
    smoke_test: Optional[bool] = field(
        default=False,
        metadata={"help": "Whether to execute in sanity check mode."}
    )
    super_smoke: Optional[bool] = field(
        default=False,
        metadata={"help": "Always run only 1 train step with max_samples={'train': 20, 'test': 25, 'dev': 15} ."}
    )
    max_train_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": "For debugging purposes or quicker training, truncate the number of training examples to this "
            "value if set."
        },
    )
    max_test_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": "For debugging purposes or quicker training, truncate the number of testing examples to this "
            "value if set."
        },
    )
    max_dev_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": "For debugging purposes or quicker training, truncate the number of dev examples to this "
            "value if set."
            },
    )
    max_seq_len: int = field(
        default=512,
        metadata={
            "help": "The maximum total input sequence length after tokenization. Sequences longer "
            "than this will be truncated, sequences shorter will be padded."
        },
    )
    preprocessing_num_workers: Optional[int] = field(
        default=None,
        metadata={"help": "The number of processes to use for the preprocessing."},
    )
    overwrite_cache: bool = field(
        default=True, metadata={"help": "Overwrite the cached training and evaluation sets."}
    )
    real_time: bool = field(
        default=False, metadata={"help": "Whether to pre-process the inputs in real-time."}
    )
    few_shot: bool = field(
        default=False,
        metadata={
            "help": "Employ few-shot pattern-based MLM training on a small subset of the data."
        },
    )
    pattern_id: int = field(
        default=0, metadata={"help": "Few-shot: pattern id of the pattern to use for few-shot training."}
    )
    label_all_tokens: bool = field(
        default=False,
        metadata={
            "help": "Whether to put the label for one word on all tokens of generated by that word or just on the "
            "one (in which case the other tokens will have a padding index)."
        },
    )
    npe_only: bool = field(
        default=False,
        metadata={
            "help": "Run step 1 only (for debug puropes)."
        },
    )
    save_model: bool = field(
        default=True,
        metadata={
            "help": "Whether to save the fine-tuned model to file."
        },
    )
    np_extractors: str = field(
        default="pos",
        metadata={
            "help": "Which Noun Phrase extraction methods to use. Specify 1 or 2 from "
            "['pos', 'regex' and 'chunker'] seperated by '+', E.g. 'pos+regex'."
        },
    )
    results_dir: Optional[str] = field(
        default=None,
        metadata={
            "help": "Directory to save all results for current test mode (model/baseline)."
        },
    )
    pos_ex_only: bool = field(
        default=False,
        metadata={
            "help": "Use only positive examples for training."
        },
    )
    max_train_steps: int = field(
        default=None,
        metadata={
            "help": "Total number of training steps to perform. If provided, overrides num_train_epochs."
        },
    )
    alpha: float = field(
        default=0.4,
        metadata={
            "help": "Alpha value for linear loss calculation."
        },
    )
    per_device_unlabeled_batch_size: int = field(
        default=4,
        metadata={
            "help": "Size of unlabeled batch for training."
        },
    )
    mlm_prob: float = field(
        default=0.15,
        metadata={
            "help": "MLM probability."
        },
    )
    tr_phases: dict = field(
        default = None,
        metadata={
            "help": "Dict describing few_shot model training phases configuration."
        },
    )

    ace_using_model: bool = field(default=None)
    ace_model: str = field(default=None)
    ace_steps: int = field(default=None)
    ace_lr: float = field(default=None)
    ace_bs: int = field(default=None)

    tr_phase_1_label_loss: bool = field(default=None)
    tr_phase_1_lm: bool = field(default=None)
    tr_phase_1_steps: int = field(default=None)
    tr_phase_1_lr: float = field(default=None)

    tr_phase_2: bool = field(default=None)
    tr_phase_2_label_loss: bool = field(default=None)
    tr_phase_2_lm: bool = field(default=None)
    tr_phase_2_steps: int = field(default=None)
    tr_phase_2_lr: float = field(default=None)
    
def verify_and_load_json_dataset(path: str):
    # Verify json schema
    with open(path) as jsonl_f:
        for line in jsonl_f:
            data = json.loads(line)
            assert all(k in data.keys() for k in ("tokens", "tags", "text"))
            assert isinstance(data["tokens"], list)
            assert isinstance(data["tags"], list)
            assert isinstance(data["text"], str)
            assert len(data["tags"]) == len(data["tokens"])

            for tok, tag in zip(data["tokens"], data["tags"]):
                assert isinstance(tok, str)
                assert isinstance(tag, str)

    print("json schema successfully verified.")
    dataset = load_dataset("json", data_files=path)
    return dataset